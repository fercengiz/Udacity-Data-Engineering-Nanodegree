{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, dayofweek,hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Dat, TimestampType\n",
    "\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://fercengiz-sparkify-dend/\"\n",
    "\n",
    "print(\"**** Starting to process song data *****\")\n",
    "# get filepath to song data file\n",
    "song_data = input_data+'song_data/*/*/*/*.json'\n",
    "\n",
    "# read song data file\n",
    "\n",
    "songSchema = R([\n",
    "    Fld(\"artist_id\",Str()),\n",
    "    Fld(\"artist_latitude\",Dbl()),\n",
    "    Fld(\"artist_location\",Str()),\n",
    "    Fld(\"artist_longitude\",Dbl()),\n",
    "    Fld(\"artist_name\",Str()),\n",
    "    Fld(\"song_id\",Str()),\n",
    "    Fld(\"duration\",Dbl()),\n",
    "    Fld(\"num_songs\",Int()),\n",
    "    Fld(\"title\",Str()),\n",
    "    Fld(\"year\",Int()),\n",
    "])\n",
    "\n",
    "\n",
    "try:\n",
    "    df = spark.read.json(song_data, schema=songSchema)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "# extract columns to create songs table\n",
    "songs_fields = [\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]\n",
    "songs_table = df.select(songs_fields)\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "try:\n",
    "    songs_table.write.parquet(output_data + \"songs.parquet\", partitionBy=(\"year\", \"artist_id\"), mode=\"overwrite\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"**** songs table data load is complete *****\")\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_fields = [\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as lattitude\", \"artist_longitude as longitude\"]\n",
    "artists_table = df.selectExpr(artists_fields)\n",
    "\n",
    "# write artists table to parquet files\n",
    "try:\n",
    "    artists_table.write.parquet(output_data + \"artists.parquet\",  mode=\"overwrite\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(\"**** artists table data load is complete *****\")\n",
    "\n",
    "print(\"**** song data processing is finished *****\")\n",
    "\n",
    "\n",
    "print(\"**** Starting to process log data *****\")\n",
    "# get filepath to log data file\n",
    "log_data = input_data+'log_data/*/*/*.json'\n",
    "\n",
    "# read log data file\n",
    "try:\n",
    "    df =spark.read.json(log_data)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == \"NextSong\")\n",
    "\n",
    "# extract columns for users table    \n",
    "users_fields = [\"userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender\", \"level\"]\n",
    "users_table = df.selectExpr(users_fields)\n",
    "\n",
    "# write users table to parquet files\n",
    "try:\n",
    "    users_table.write.parquet(output_data + \"users.parquet\",  mode=\"overwrite\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"**** users table data load is complete *****\")\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = udf(date_convert, TimestampType())\n",
    "#df = df.withColumn(\"datetime\",get_timestamp(df.ts))\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda ms: datetime.fromtimestamp(ms // 1000), TimestampType())\n",
    "df = df.withColumn(\"datetime\",get_timestamp(df.ts))\n",
    "   \n",
    "# extract columns to create time table\n",
    "time_fields = [\"datetime as start_time\", \"hour(datetime) as hour\", \"dayofmonth(datetime) as day\",\n",
    "               \"weekofyear(datetime) as week\", \"month(datetime) as month\", \"year(datetime) as year\", \n",
    "               \"dayofweek(datetime) as weekday\"]\n",
    "time_table = df.select(time_fields)\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "try:\n",
    "    time_table.write.parquet(output_data + \"time.parquet\", partitionBy=(\"year\", \"month\"), mode=\"overwrite\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"**** time table data load is complete *****\")\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "songs_df = spark.read.parquet(output_data + \"songs.parquet\")\n",
    "\n",
    "artists_df = spark.read.parquet(output_data + \"artists.parquet\")\n",
    "\n",
    "song_df = songs_df.join(artists_df.aslias(\"artists\"), \n",
    "                        songs_df.artist_id == artists_df.artist_id , \n",
    "                        \"inner\" ).select(\"title\", \"name\", \"duration\", \"song_id\", \"artists.artist_id\")\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df.join(song_df , (df.song == song_df.title) & (df.artist ==song_df.name) & (df.length == song_df.duration), \"inner\")\n",
    "songplays_table = songplays_table.withColumn(\"songplay_id\",monotonically_increasing_id())\n",
    "songplays_table = songplays_table.selectExpr(\"songplay_id\", \"datetime as start_time\", \"userId as user_id\",\n",
    "                                             \"month(datetime) as month\", \"year(datetime) as year\",\n",
    "                                             \"level\", \"song_id\", \"artist_id\",\"sessionId as session_id\",\n",
    "                                             \"location\", \"userAgent as user_agent\")\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "try:\n",
    "    songplays_table.write.parquet(output_data + \"songplays.parquet\", partitionBy=(\"year\", \"month\"), mode=\"overwrite\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"**** songplays table data load is complete *****\")\n",
    "\n",
    "print(\"**** log data processing is finished *****\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
